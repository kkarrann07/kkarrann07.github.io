<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>ChechIT - AI Fashion Try-On (MediaPipe BlazePose + OpenCV perspective warp)</title>
<style>
  body {
    font-family: Arial, sans-serif;
    text-align: center;
    background: #f7f7f7;
    margin: 0; padding: 0;
  }
  header {
    background: #222; color: white; padding: 1rem; font-weight: bold;
  }
  .ar-container {
    position: relative;
    width: 640px;
    height: 480px;
    margin: 1rem auto;
    border-radius: 12px;
    overflow: hidden;
    background: black;
  }
  video, canvas {
    position: absolute;
    top: 0; left: 0;
    width: 640px;
    height: 480px;
    border-radius: 12px;
  }
  #video {
    transform: none; /* No mirror */
  }
  #canvas {
    z-index: 10;
  }
  #upload {
    margin: 1rem;
  }
</style>
</head>
<body>

<header>ðŸ‘— ChechIT â€“ AI Fashion Try-On with BlazePose + OpenCV Perspective Warp</header>

<input type="file" id="upload" accept="image/*" />

<div class="ar-container">
  <video id="video" autoplay playsinline></video>
  <canvas id="canvas" width="640" height="480"></canvas>
</div>

<script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose@0.4/pose.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils@0.4/camera_utils.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils@0.4/drawing_utils.js"></script>
<script async src="https://docs.opencv.org/4.x/opencv.js"></script>

<script>
  let video = document.getElementById('video');
  let canvas = document.getElementById('canvas');
  let ctx = canvas.getContext('2d');
  let shirtImage = new Image();
  let shirtLoaded = false;

  // Load uploaded shirt image
  document.getElementById('upload').addEventListener('change', e => {
    const file = e.target.files[0];
    if (!file) return;
    const reader = new FileReader();
    reader.onload = ev => {
      shirtImage.src = ev.target.result;
    };
    reader.readAsDataURL(file);
  });
  shirtImage.onload = () => {
    shirtLoaded = true;
  };

  // Wait for OpenCV to load
  let cvReady = false;
  function onOpenCvReady() {
    cvReady = true;
    console.log('OpenCV ready');
  }
  if (typeof cv !== 'undefined') {
    cv['onRuntimeInitialized']=onOpenCvReady;
  }

  // Setup MediaPipe BlazePose
  const pose = new Pose.Pose({
    locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/pose@0.4/${file}`
  });
  pose.setOptions({
    modelComplexity: 2, // Full model, highest accuracy
    smoothLandmarks: true,
    enableSegmentation: false,
    smoothSegmentation: false,
    minDetectionConfidence: 0.7,
    minTrackingConfidence: 0.7
  });

  pose.onResults(onResults);

  // Start camera with MediaPipe camera utils (no mirror)
  const mpCamera = new CameraUtils.Camera(video, {
    onFrame: async () => {
      await pose.send({image: video});
    },
    width: 640,
    height: 480
  });
  mpCamera.start();

  function onResults(results) {
    if (!cvReady || !shirtLoaded) {
      // Just draw the video frame if not ready
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.drawImage(results.image, 0, 0, canvas.width, canvas.height);
      return;
    }

    // Draw the video frame first
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(results.image, 0, 0, canvas.width, canvas.height);

    if (!results.poseLandmarks) return;

    // Get key landmarks needed for torso mapping
    const landmarks = results.poseLandmarks;

    // Landmarks indexes:
    // Left shoulder = 11
    // Right shoulder = 12
    // Left hip = 23
    // Right hip = 24

    const leftShoulder = landmarks[11];
    const rightShoulder = landmarks[12];
    const leftHip = landmarks[23];
    const rightHip = landmarks[24];

    // Check confidence threshold (z or visibility could also be checked)
    if (
      leftShoulder.visibility < 0.5 || rightShoulder.visibility < 0.5 ||
      leftHip.visibility < 0.5 || rightHip.visibility < 0.5
    ) {
      return; // skip if landmarks not confident enough
    }

    // Convert normalized coordinates to pixel coords
    function normToPx(pt) {
      return {
        x: pt.x * canvas.width,
        y: pt.y * canvas.height
      };
    }

    let ptsDst = [
      normToPx(leftShoulder),  // top-left shirt corner
      normToPx(rightShoulder), // top-right shirt corner
      normToPx(rightHip),      // bottom-right shirt corner
      normToPx(leftHip)        // bottom-left shirt corner
    ];

    // Now prepare OpenCV Mats for perspective transform

    // Source points on the shirt image (corners)
    let ptsSrc = cv.matFromArray(4, 1, cv.CV_32FC2, [
      0, 0,
      shirtImage.width, 0,
      shirtImage.width, shirtImage.height,
      0, shirtImage.height
    ]);
    // Destination points on canvas (detected torso)
    let ptsDstCv = cv.matFromArray(4, 1, cv.CV_32FC2, [
      ptsDst[0].x, ptsDst[0].y,
      ptsDst[1].x, ptsDst[1].y,
      ptsDst[2].x, ptsDst[2].y,
      ptsDst[3].x, ptsDst[3].y
    ]);

    // Read shirt image into OpenCV Mat
    let srcMat = cv.imread(shirtImage);

    // Destination Mat: blank transparent canvas
    let dstMat = new cv.Mat.zeros(canvas.height, canvas.width, cv.CV_8UC4);

    // Get perspective transform matrix
    let M = cv.getPerspectiveTransform(ptsSrc, ptsDstCv);

    // Warp shirt image to destination area on canvas
    cv.warpPerspective(srcMat, dstMat, M, new cv.Size(canvas.width, canvas.height), cv.INTER_LINEAR, cv.BORDER_CONSTANT, new cv.Scalar(0,0,0,0));

    // Convert result to ImageData and draw on canvas overlay
    let imgData = new ImageData(new Uint8ClampedArray(dstMat.data), dstMat.cols, dstMat.rows);
    ctx.putImageData(imgData, 0, 0);

    // Cleanup
    ptsSrc.delete();
    ptsDstCv.delete();
    srcMat.delete();
    dstMat.delete();
    M.delete();
  }
</script>

</body>
</html>
